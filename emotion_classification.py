# -*- coding: utf-8 -*-
"""Emotion classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13mvU8NI7hb1m2jLh7x3ApPlCAs9x0KBc
"""

from google.colab import files
uploaded = files.upload()

pip install snscrape

import pandas as pd
import re
import string
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.cluster import KMeans
import subprocess

import json

def scrape_tweets(keyword, max_tweets=100):
    output_file = "scraped_tweets.json"
    command = f"snscrape --max-results {max_tweets} --jsonl twitter-search '{keyword}' > {output_file}"
    subprocess.run(command, shell=True)

    tweets = []
    with open(output_file, 'r') as f:
        for line in f:
            data = json.loads(line)
            if 'content' in data:
                tweets.append(data['content'])

    return tweets

# 2. Preprocessing function
stopwords = set([
    'i','me','my','myself','we','our','you','your','yours','he','him','his','she','her',
    'it','its','they','them','their','what','which','who','this','that','these','those',
    'am','is','are','was','were','be','been','have','has','do','does','did','a','an','the',
    'and','but','if','or','because','as','until','while','of','at','by','for','with',
    'about','against','between','into','through','before','after','above','below','to',
    'from','up','down','in','out','on','off','over','under','again','further','then',
    'once','here','there','when','where','why','how','all','any','both','each','few','more',
    'most','other','some','such','no','nor','not','only','own','same','so','than','too',
    'very','s','t','can','will','just','don','should','now'
])

def clean_text(text):
    text = str(text).lower()
    text = re.sub(r"http\S+|www\S+|@\w+|#\w+", '', text)
    text = text.translate(str.maketrans('', '', string.punctuation))
    return ' '.join([word for word in text.split() if word not in stopwords])

# 3. Load reduced dataset (50 samples per emotion)
df = pd.read_csv("/content/emotions.csv.zip")
min_count = 10000
df = (
    df.groupby('label', group_keys=False)
      .apply(lambda x: x.sample(min_count, random_state=42))
      .reset_index(drop=True)
)


df['clean_text'] = df['text'].apply(clean_text)

# 4. Vectorize
tfidf = TfidfVectorizer(max_features=1000)
X = tfidf.fit_transform(df['clean_text'])
y = df['label']

# 5. Clustering (optional unsupervised insight)
print("\n🔍 Clustering tweets into 6 groups...")
kmeans = KMeans(n_clusters=6, random_state=42)
clusters = kmeans.fit_predict(X)
df['cluster'] = clusters

# 6. Split and train SVM
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = SVC(kernel='linear')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

print("\n📊 Classification Report:\n", classification_report(y_test, y_pred))

# 7. Save model and vectorizer
joblib.dump(model, 'svm_emotion_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

# 8. Optional: Use scraped tweets for prediction
tweets = [
    "I feel great today!",
    "Why is everything so hard?",
    "I'm excited about my trip!",
    "Feeling empty and alone.",
    "What an amazing experience!"
]

cleaned_tweets = [clean_text(tweet) for tweet in tweets]

if len(cleaned_tweets) == 0:
    print("❗ No tweets found. Try a different keyword or check internet access.")
else:
    tweet_vectors = tfidf.transform(cleaned_tweets)
    predictions = model.predict(tweet_vectors)

    # 9. Alert system (prints alert if emotion is '4' = sad)
    for i, pred in enumerate(predictions):
        if pred == 4:
            print(f"🚨 Alert: Sad tweet detected -> {tweets[i]}")
        else:
            print(f"🙂 Emotion detected ({pred}): {tweets[i]}")